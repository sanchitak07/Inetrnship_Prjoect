# Project: Restaurant Location Recommendation Tool & Analysis ##

# Goal
To preprocess restaurant dataset features by encoding categorical variables and scaling numerical features, ensuring all inputs are in a suitable format for machine learning models and improving model performance and convergence.

# Objective
To prepare the dataset for machine learning models by:
 1. Converting categorical variables into numerical format using Label Encoding.
 2. Scaling the numerical features using two popular methods:
      Min-Max Normalization
      Standardization (Z-score Scaling)

Feature scaling ensures that all input features contribute equally to the model’s learning process and prevents features with larger numerical ranges from dominating others.

# Why Feature Scaling?
Many machine learning algorithms (e.g., logistic regression, KNN, SVM, gradient descent-based models) are sensitive to the scale of input features. If features vary widely in magnitude, models may converge slowly or yield inaccurate results.

# Feature Scaling Steps

1. Import Required Libraries

<pre>from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split </pre>

MinMaxScaler: Used for normalization (scales values to [0, 1]).

StandardScaler: Used for standardization (makes mean = 0, std = 1).

train_test_split: (not used here but often used to split data into training and test sets).

2. Split Features and Target

<pre>X = dataset.drop(columns=['Name', 'Rating'], errors='ignore') </pre>

Removes columns 'Name' and 'Rating' from your dataset.

X now contains only independent input features (things the model will learn from).

<pre>y = dataset['Rating'] </pre>

y stores the target variable — the value the model is trying to predict (restaurant rating).

<pre>print("Independent Features (X):\n", X.columns.tolist())
print("Dependent Variable (y): Rating") </pre>

This shows all column names in X, and confirms the target variable y is 'Rating'.

3. Label Encoding for Categorical Features

<pre>from sklearn.preprocessing import LabelEncoder </pre>

Imports the label encoder, which converts text categories into numbers.

<pre>X_encoded = X.copy() </pre>

Makes a copy of X so we don’t change the original data.

<pre>le = LabelEncoder() </pre>

Creates an encoder object called le.

<pre>for column in X_encoded.select_dtypes(include='object').columns:
    X_encoded[column] = le.fit_transform(X_encoded[column].astype(str)) </pre>

Loops through all text (object type) columns.

Converts each category into a number using Label Encoding.

<pre>print("Label Encoding completed!")
print(X_encoded.head()) </pre>

Confirms encoding is done and prints the first 5 rows of the new encoded data.

4. Normalization (Min-Max Scaling)

<pre>scaler_norm = MinMaxScaler() </pre>

Creates a Min-Max scaler object.

<pre>X_normalized = scaler_norm.fit_transform(X_encoded) 
print(" Normalization (Min-Max Scaling) done!") </pre>

Scales all feature values to be between 0 and 1.

Confirms that normalization is complete.

5. Standardization (Z-score Scaling)

<pre>scaler_std = StandardScaler() </pre>

Creates a StandardScaler object.

<pre>X_standardized = scaler_std.fit_transform(X_encoded) </pre>

Scales features so that:

   1. Mean = 0
   2. Standard deviation = 1

<pre>print(" Standardization (Z-score Scaling) done!") </pre>

Confirms that standardization is complete.

6. Convert to DataFrames for Better Viewing

<pre>X_norm_df = pd.DataFrame(X_normalized, columns=X_encoded.columns)
X_std_df = pd.DataFrame(X_standardized, columns=X_encoded.columns) </pre>

Converts the scaled arrays back to pandas DataFrames with original column names.

<pre>print("Normalized Data Sample:")
print(X_norm_df.head()) </pre>

Prints top 5 rows of normalized data.

<pre>print("\n Standardized Data Sample:")
print(X_std_df.head()) </pre>

Prints top 5 rows of standardized data.

# Summary
 
In this process, we prepared our restaurant dataset for machine learning by first separating the input features (X) and target variable (y). Categorical features such as "Footfall_Peak" and "Parking_Available" were converted into numerical form using Label Encoding to make them suitable for model input. Then, two popular feature scaling techniques were applied:

Min-Max Normalization, which scales all feature values to a fixed range between 0 and 1, and

Standardization (Z-score Scaling), which transforms features to have a mean of 0 and a standard deviation of 1.

The result was two clean, numerical datasets (X_normalized and X_standardized) that are now ready for training machine learning models. These transformations ensure that features contribute equally to model learning and improve algorithm convergence and performance.










